now we convert this simple service to microservice and up it with docker compose:


Ù…Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒÙ… Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ… Ø¨Ù‡ ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ú†Ù†Ø¯-Ø³Ø±ÙˆÛŒØ³ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ Ø§ÛŒÙ† Ø§Ø¬Ø²Ø§:


| Ø³Ø±ÙˆÛŒØ³                 | ØªÙˆØ¶ÛŒØ­                                                         |
| --------------------- | ------------------------------------------------------------- |
| `model-api`           | Ø³Ø±ÙˆÛŒØ³ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ GPT Ø±Ùˆ Ø¨Ø§ FastAPI Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡                  |
| `frontend`            | Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ (Ù…Ø«Ù„Ø§Ù‹ Streamlit ÛŒØ§ HTML Ø³Ø§Ø¯Ù‡) Ø¨Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ Ù…Ø¯Ù„  |
| `logger`              | ÛŒÚ© Ø³Ø±ÙˆÛŒØ³ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù„Ø§Ú¯â€ŒÙ‡Ø§                               |
| `monitor`             | Ø§Ø¨Ø²Ø§Ø± Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ â€“ Ù…Ø«Ù„Ø§Ù‹ Prometheus + Grafana Ø¨Ø¹Ø¯Ø§Ù‹) |
| `docker-compose.yaml` | Ø¨Ø±Ø§ÛŒ orkestration ØªÙ…Ø§Ù… Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§                               |





Ú¯Ø§Ù… 1: Ø³Ø§Ø®Øª Ø³Ø±ÙˆÛŒØ³ model-api Ø¨Ø§ FastAPI
Ù…Ø§ Ø§Ù„Ø§Ù† Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒÙ… ÛŒÚ© API Ø¨Ø³Ø§Ø²ÛŒÙ… Ú©Ù‡:

Ù…Ø¯Ù„ GPT Ú©ÙˆÚ†ÛŒÚ© (Ù…Ø«Ù„Ø§Ù‹ sshleifer/tiny-gpt2) Ø±Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†Ù‡

Ø§Ø² Ø·Ø±ÛŒÙ‚ HTTP Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ú¯ÛŒØ±Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Ù…ØªÙ† Ú©Ø§Ø±Ø¨Ø±) Ùˆ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡

ğŸ“ Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ
bash
Copy
Edit
mini-gpt-lab/
â”œâ”€â”€ model-api/
â”‚   â”œâ”€â”€ app.py              # Ú©Ø¯ FastAPI
â”‚   â”œâ”€â”€ requirements.txt    # Ù„ÛŒØ³Øª Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§
â”‚   â””â”€â”€ Dockerfile          # Dockerfile Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ API
â”œâ”€â”€ docker-compose.yaml     # ØªØ¹Ø±ÛŒÙ ØªÙ…Ø§Ù… Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
ğŸ§  Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
1. model-api/app.py
python
Copy
Edit
from fastapi import FastAPI, Request
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

app = FastAPI()

tokenizer = AutoTokenizer.from_pretrained("sshleifer/tiny-gpt2")
model = AutoModelForCausalLM.from_pretrained("sshleifer/tiny-gpt2").cuda()

class Prompt(BaseModel):
    text: str

@app.post("/generate")
def generate(prompt: Prompt):
    inputs = tokenizer.encode(prompt.text, return_tensors="pt").cuda()
    outputs = model.generate(inputs, max_length=50)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"response": response}
2. model-api/requirements.txt
txt
Copy
Edit
fastapi
uvicorn
transformers
torch
3. model-api/Dockerfile
Dockerfile
Copy
Edit
FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04

RUN apt update && apt install -y python3 python3-pip

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY app.py .

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
4. docker-compose.yaml
yaml
Copy
Edit
version: "3.9"

services:
  model-api:
    build: ./model-api
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
ğŸš€ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡
bash
Copy
Edit
docker compose up --build
âœ… ØªØ³Øª API
Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø± ÛŒØ§ Ø¨Ø§ curl:

bash
Copy
Edit
curl -X POST http://localhost:8000/generate -H "Content-Type: application/json" -d '{"text": "Hello, my name is"}'




